"use strict";(self.webpackChunktest=self.webpackChunktest||[]).push([[731],{7309:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>c});var t=r(4848),s=r(8453);const i={sidebar_position:2,title:"Requirements"},a="Minimum Requirements",o={id:"getting-started/requirements",title:"Requirements",description:"The Observability Stack requires a minimum one Kubernetes cluster to initiate the Observer Cluster and Fleet components for self-observation. However, it's best to keep the Fleet components separate from the Observer Cluster. An easy method to achieve this separation is by utilizing the Rancher Cluster Manager which is integrated with Fleet. This setup allows for efficient management of downstream clusters and the deployment and administration of Observability stack components. In the following chapter, we will discuss the minimum requirements needed to establish a single-node Rancher Cluster Manager setup.",source:"@site/docs/getting-started/requirements.md",sourceDirName:"getting-started",slug:"/getting-started/requirements",permalink:"/getting-started/requirements",draft:!1,unlisted:!1,editUrl:"https://github.com/observability-stack/observability-stack.io/edit/main/docs/docs/getting-started/requirements.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,title:"Requirements"},sidebar:"tutorialSidebar",previous:{title:"Glossary",permalink:"/getting-started/glossary"},next:{title:"Quickstart",permalink:"/getting-started/quickstart"}},l={},c=[{value:"Hosting Rancher Cluster Manager",id:"hosting-rancher-cluster-manager",level:2},{value:"Hardware",id:"hardware",level:3},{value:"Software",id:"software",level:3},{value:"Observer Cluster",id:"observer-cluster",level:2},{value:"Hardware",id:"hardware-1",level:3},{value:"Software",id:"software-1",level:3},{value:"Operator Lifecycle Manager (OLM)",id:"operator-lifecycle-manager-olm",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"minimum-requirements",children:"Minimum Requirements"}),"\n",(0,t.jsxs)(n.p,{children:["The Observability Stack requires a minimum one Kubernetes cluster to initiate the Observer Cluster and Fleet components for self-observation. However, it's best to keep the Fleet components separate from the Observer Cluster. An easy method to achieve this separation is by utilizing the ",(0,t.jsx)(n.a,{href:"https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/deploy-apps-across-clusters/fleet",children:"Rancher Cluster Manager which is integrated with Fleet"}),". This setup allows for efficient management of downstream clusters and the deployment and administration of Observability stack components. In the following chapter, we will discuss the minimum requirements needed to establish a single-node Rancher Cluster Manager setup."]}),"\n",(0,t.jsx)(n.h2,{id:"hosting-rancher-cluster-manager",children:"Hosting Rancher Cluster Manager"}),"\n",(0,t.jsx)(n.p,{children:"The Observability Stack is designed to work well with the Rancher Cluster Manager, making good use of its Fleet integration for setting up components and updating configurations The hardware specifications required for the Rancher Cluster Manager depends on how many clusters you need to control."}),"\n",(0,t.jsxs)(n.p,{children:["For starting with a setup that involves just one node, the ",(0,t.jsx)(n.a,{href:"https://ranchermanager.docs.rancher.com/getting-started/installation-and-upgrade/installation-requirements#k3s-kubernetes",children:"baseline requirements"})," which can manage up to 150 clusters are as follows:"]}),"\n",(0,t.jsx)(n.h3,{id:"hardware",children:"Hardware"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"CPU"}),": 4 vCPUs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Memory"}),": 16GB RAM"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Storage"}),": 40GB (SSDs are recommended)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Network"}),": Each node used should have a static IP configured, regardless of whether you are installing Rancher on a single node or on an HA cluster. In case of DHCP, each node should have a DHCP reservation to make sure the node gets the same IP allocated. Rancher also requires a ",(0,t.jsx)(n.a,{href:"https://ranchermanager.docs.rancher.com/getting-started/installation-and-upgrade/installation-requirements/port-requirements",children:"number of ports"})," to be open to communicate with the downstream clusters."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"software",children:"Software"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Operating System"}),": A modern Linux distribution (e.g., Ubuntu 22.04 LTS)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Kubernetes"}),": A lightweight Kubernetes installation such as k3s."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Helm"}),": Version 3.x for deploying Rancher"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["For a comprehensive understanding and guidance on deploying the Rancher Cluster Manager, please consult the ",(0,t.jsx)(n.a,{href:"https://ranchermanager.docs.rancher.com/getting-started/quick-start-guides/deploy-rancher-manager/helm-cli",children:"official Rancher documentation"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["To explore the intricacies of Fleet and its role in the Observability Stack, visit the ",(0,t.jsx)(n.a,{href:"#hosting-rancher-cluster-manager",children:"best practices guide on Rancher"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["Futher details for integrating the Rancher Cluster Manager and Fleet are available in ",(0,t.jsx)(n.a,{href:"../catagory/how-to-guides/rancher-cluster-manager",children:"Rancher Cluster Manager and Fleet Reference Architecture Guide"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"observer-cluster",children:"Observer Cluster"}),"\n",(0,t.jsx)(n.p,{children:"Observer cluster, by desing, more resource-intensive than the observee clusters. This is because it runs extra applications on top of the standard application stack found in observee clusters. These additional applications include a central Opensearch cluster for logging and tracing and Thanos for metrics data. Observer cluster will also be the focal entry for longer term observability data archiving and queriying. Therefore, the resources needed for the Observer cluster will vary depending on several factors, like the number of clusters it will monitor and the data retention period."}),"\n",(0,t.jsx)(n.p,{children:"For a basic setup, a 3-node Kubernetes cluster equipped with the following specifications should suffice:"}),"\n",(0,t.jsx)(n.h3,{id:"hardware-1",children:"Hardware"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"CPU"}),": 8 cores (16 cores recommended)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Memory"}),": 16GB RAM (32GB recommended)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Storage"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"100GB block storage for the root filesystem"}),"\n",(0,t.jsx)(n.li,{children:"Additional CSI-based network-attached storage for observability data"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Network"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"HTTPS connectivity between observee and observer clusters"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"software-1",children:"Software"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Operating System"}),": A modern Linux distribution (e.g., Ubuntu 22.04 LTS)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Kubernetes"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Version 1.20 or newer"}),"\n",(0,t.jsx)(n.li,{children:"Operator Lifecylce Manager (OLM) version 0.20.0"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Storage"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Support for dynamic volume provisioning with StorageClasses"}),"\n",(0,t.jsx)(n.li,{children:"Manual Persistent Volume (PV) creation capability if necessary"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Network"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"An Ingress controller to expose services"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note"}),": Using the root filesystem for observability data via hostPath or CSI solutions like Longhorn has not been tested extensively and may lead to node instability. It's recommended to use dedicated storage solutions to prevent potential node crashes."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"operator-lifecycle-manager-olm",children:"Operator Lifecycle Manager (OLM)"}),"\n",(0,t.jsx)(n.p,{children:"To be complated."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var t=r(6540);const s={},i=t.createContext(s);function a(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);